{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import configparser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file():\n",
    "    \"\"\"\n",
    "    Read configuration file\n",
    "    :return:\n",
    "    RawConfigParser with config data inside\n",
    "    \"\"\"\n",
    "    import configparser\n",
    "    # Read configuration file\n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read('../configs/param.cfg')\n",
    "    # print(f'ACCESS_KEY_ID: {config[\"AWS_CREDENTIAL\"][\"AWS_ACCESS_KEY_ID\"]}\\n\\\n",
    "    # SECRET_ACCESS_KEY: {config[\"AWS_CREDENTIAL\"][\"AWS_SECRET_ACCESS_KEY\"]}\\n\\\n",
    "    # SESSION_TOKEN: {config[\"AWS_CREDENTIAL\"][\"AWS_SESSION_TOKEN\"]}')\n",
    "    return config\n",
    "\n",
    "def print_log(message):\n",
    "    \"\"\"\n",
    "    Log into console datetime to execute along with message\n",
    "    param:\n",
    "        message: [String] Message to print in the console\n",
    "    \"\"\"\n",
    "    print('{} - '.format(datetime.now()) + message)\n",
    "\n",
    "def download_data(config, data_name):\n",
    "    \"\"\"\n",
    "    Send 1 request to data sources and download data\n",
    "    :return:\n",
    "    none\n",
    "    save downloaded file into cluster hard disk\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    path_question = config['FILE_LOCATION'][f'loc_so_{data_name}']\n",
    "    url_question = config['SOURCE_URL'][f'so_{data_name}_zip']\n",
    "    req_question = requests.get(url_question, allow_redirects=True)\n",
    "    print_log(f'{data_name}.zip downloaded')\n",
    "    open(path_question, 'wb').write(req_question.content)\n",
    "    print_log(f'{data_name}.zip saved to local')\n",
    "\n",
    "\n",
    "def add_hash_column(config, data_name):\n",
    "    \"\"\"\n",
    "    This function will add a column hashing all data of each row,\n",
    "    this column will be used to track changes in data source in FUTURE USE\n",
    "    :return:\n",
    "    none\n",
    "    data passed in this function will be added hash column and save as CSV file\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    path_data = config['FILE_LOCATION'][f'loc_so_{data_name}']\n",
    "    path_output = config['FILE_LOCATION'][f'loc_{data_name}_with_hash']\n",
    "    df = pd.read_csv(path_data, compression='zip', nrows=1000000)\n",
    "\n",
    "    # Add hash_key column to track data changes in each row\n",
    "    df['hash_key'] = df.apply(lambda row: pd.util.hash_pandas_object(\n",
    "        pd.Series(row.to_string())), axis=1)\n",
    "    print_log(f'hash_key for {data_name} table created')\n",
    "\n",
    "    df.to_csv(path_output)\n",
    "    print_log(f'{data_name} table with hash_key saved into local')\n",
    "\n",
    "def transform_question_data(config):\n",
    "    \"\"\"\n",
    "    This function will create dimDate table by extracting from [questions] table\n",
    "    :param config: environment configuration\n",
    "    :return:\n",
    "    none\n",
    "    dimDate table created as CSV file\n",
    "    factQuestion will be updated by adding 3 date columns, renaming 3 datetime columns\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    path_question = config['FILE_LOCATION']['loc_question_with_hash']\n",
    "    path_dim_date = config['FILE_LOCATION']['loc_dim_date']\n",
    "    path_fact_question = config['FILE_LOCATION']['loc_fact_question']\n",
    "    print_log(f'Done reading file location config')\n",
    "\n",
    "    df_question = pd.read_csv(path_question)\n",
    "    print_log(f'Done reading question_with_hash csv')\n",
    "\n",
    "    # BEGIN: Create dim_date table\n",
    "    # - Get all date columns in question table and exclude time in datetime data\n",
    "    df_date_stg = df_question[['CreationDate', 'ClosedDate', 'DeletionDate']].apply(\n",
    "        lambda date: pd.to_datetime(date).dt.date, axis=1)\n",
    "\n",
    "    # - Combine 3 date columns into one,, remove null and duplicates\n",
    "    sr_date = pd.concat([df_date_stg['CreationDate'], df_date_stg['ClosedDate'], df_date_stg['DeletionDate']],\n",
    "                        axis=0,\n",
    "                        ignore_index=True).dropna().drop_duplicates().sort_values()\n",
    "\n",
    "    # - Create dim_date dataframe, add relative columns\n",
    "    df_date = pd.DataFrame({'date': sr_date,\n",
    "                            'day': sr_date.apply(lambda d: d.day),\n",
    "                            'weekday': sr_date.apply(lambda d: d.weekday()),\n",
    "                            'month': sr_date.apply(lambda d: d.month),\n",
    "                            'quarter': sr_date.apply(lambda d: int(np.ceil(d.month / 3))),\n",
    "                            'year': sr_date.apply(lambda d: d.year)}).reset_index(drop=True)\n",
    "    df_date.to_csv(path_dim_date)\n",
    "    print_log(f'dimDate table saved into local')\n",
    "    # END: Create dim_date table\n",
    "\n",
    "    # BEGIN: Transform fact_question table\n",
    "    # - Rename time in datetime columns\n",
    "    df_question.rename(columns={'CreationDate': 'CreationDateTime', 'ClosedDate': 'ClosedDateTime',\n",
    "                                'DeletionDate': 'DeletionDateTime'}, inplace=True)\n",
    "    df_question = pd.concat([df_question, df_date_stg], axis=1)\n",
    "    print_log(f'question table updated - adding 3 date columns, rename 3 datetime columns')\n",
    "\n",
    "    # - Add status column in fact_question table\n",
    "    df_question['status'] = df_question.apply(lambda row:\n",
    "                                              'Closed' if isinstance(row['ClosedDate'], str) else\n",
    "                                              'Deleted' if isinstance(row['DeletionDate'], str) else\n",
    "                                              'Open', axis=1)\n",
    "    df_question.to_csv(path_fact_question)\n",
    "    print_log(f'factQuestion table saved into local')\n",
    "    # END: Transform fact_question table\n",
    "\n",
    "\n",
    "def transform_question_tag_data(config):\n",
    "    \"\"\"\n",
    "    This function will create dimTag table which is extracted from [question tags] data\n",
    "    :param config: environment configuration\n",
    "    :return:\n",
    "    none\n",
    "    dimTag table created as csv file and factQuestionTag table updated by adding FK from dimTag table\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    path_question_tag = config['FILE_LOCATION']['loc_question_tag_with_hash']\n",
    "    path_fact_question_tag = config['FILE_LOCATION']['loc_fact_question_tag']\n",
    "    path_dim_tag = config['FILE_LOCATION']['loc_dim_tag']\n",
    "    print_log('file location config read successful')\n",
    "\n",
    "    df_question_tag = pd.read_csv(path_question_tag)\n",
    "\n",
    "    # START: Create dim_tag table\n",
    "    # - Get the column Tag in question_tag table\n",
    "    df_tag = df_question_tag['Tag'].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "\n",
    "    # - Rename column for business understanding\n",
    "    df_tag.rename(columns={'index': 'TagID'}, inplace=True)\n",
    "    print_log('dimTag table created')\n",
    "    df_tag.to_csv(path_dim_tag)\n",
    "    print_log('dimTag table saved into local')\n",
    "    # END: Create dim_tag table\n",
    "\n",
    "    # START: Transform fact_question_tag table\n",
    "    df_question_tag = df_question_tag.merge(df_tag, on='Tag')\n",
    "    df_question_tag.drop('Tag', axis=1, inplace=True)\n",
    "    df_question_tag.rename({'ID': 'QuestionID'}, inplace=True)\n",
    "    df_question_tag.to_csv(path_fact_question_tag)\n",
    "    print_log('factQuestionTag table updated and saved into local')\n",
    "    # END: Transform fact_question_tag table\n",
    "\n",
    "\n",
    "def push_data_to_s3(config):\n",
    "    \"\"\"\n",
    "    This function will push processed CSV files into S3 bucket\n",
    "    :param config: environment configuration\n",
    "    :return:\n",
    "    none, 6 csv files will be pushed into S3 bucket\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create AWS Credential Session\n",
    "    session = boto3.session.Session(aws_access_key_id=config[\"AWS_CREDENTIAL\"][\"AWS_ACCESS_KEY_ID\"],\n",
    "                                    aws_secret_access_key=config[\"AWS_CREDENTIAL\"][\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                                    aws_session_token=config[\"AWS_CREDENTIAL\"][\"AWS_SESSION_TOKEN\"])\n",
    "    print_log('authenticated session to S3 created')\n",
    "\n",
    "    # Put objects into S3 datalake\n",
    "    # Create resource and object\n",
    "    s3 = session.resource('s3')\n",
    "    obj_questions_raw = s3.Object(bucket_name='so-question-dl', key='raw/questions_hash.csv')\n",
    "    obj_questions_tag_raw = s3.Object(bucket_name='so-question-dl', key='raw/question_tags_hash.csv')\n",
    "    obj_questions = s3.Object(bucket_name='so-question-dl', key='factQuestion.csv')\n",
    "    obj_question_tags = s3.Object(bucket_name='so-question-dl', key='factQuestionTag.csv')\n",
    "    obj_date = s3.Object(bucket_name='so-question-dl', key='dimDate.csv')\n",
    "    obj_tag = s3.Object(bucket_name='so-question-dl', key='dimTag.csv')\n",
    "    print_log('objects in S3 located')\n",
    "\n",
    "    # Get location of csv files which will be pushed into S3\n",
    "    path_questions = config['FILE_LOCATION']['loc_question_with_hash']\n",
    "    path_question_tags = config['FILE_LOCATION']['loc_question_tag_with_hash']\n",
    "    path_dim_date = config['FILE_LOCATION']['loc_dim_date']\n",
    "    path_dim_tag = config['FILE_LOCATION']['loc_dim_tag']\n",
    "    path_fact_question = config['FILE_LOCATION']['loc_fact_question']\n",
    "    path_fact_question_tag = config['FILE_LOCATION']['loc_fact_question_tag']\n",
    "    print_log('processed csv files loaded from local')\n",
    "\n",
    "    # Put [questions.csv] object into bucket\n",
    "    obj_questions_raw.put(Body=open(path_questions, 'rb'))\n",
    "    print_log(f'{path_questions} uploaded')\n",
    "\n",
    "    # Put [question_tags.csv] object into bucket\n",
    "    obj_questions_tag_raw.put(Body=open(path_question_tags, 'rb'))\n",
    "    print_log(f'{path_question_tags} uploaded')\n",
    "\n",
    "    # Put [dimDate.csv] object into bucket\n",
    "    obj_date.put(Body=open(path_dim_date, 'rb'))\n",
    "    print_log(f'{path_dim_date} uploaded')\n",
    "\n",
    "    # Put [dimTag.csv] object into bucket\n",
    "    obj_tag.put(Body=open(path_dim_tag, 'rb'))\n",
    "    print_log(f'{path_dim_tag} uploaded')\n",
    "\n",
    "    # Put [factQuestion.csv] object into bucket\n",
    "    obj_questions.put(Body=open(path_fact_question, 'rb'))\n",
    "    print_log(f'{path_fact_question} uploaded')\n",
    "\n",
    "    # Put [factQuestionTag.csv] object into bucket\n",
    "    obj_question_tags.put(Body=open(path_fact_question_tag, 'rb'))\n",
    "    print_log(f'{path_fact_question_tag} uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 22:57:48.643191 - question.zip downloaded\n",
      "2021-11-15 22:57:48.931492 - question.zip saved to local\n",
      "2021-11-15 22:58:12.710377 - question_tag.zip downloaded\n",
      "2021-11-15 22:58:13.045308 - question_tag.zip saved to local\n",
      "2021-11-15 23:10:09.436749 - hash_key for question table created\n",
      "2021-11-15 23:10:12.986345 - question table with hash_key saved into local\n",
      "2021-11-15 23:21:51.774008 - hash_key for question_tag table created\n",
      "2021-11-15 23:21:54.988365 - question_tag table with hash_key saved into local\n",
      "2021-11-15 23:21:54.988365 - Done reading file location config\n",
      "2021-11-15 23:21:56.741681 - Done reading question_with_hash csv\n",
      "2021-11-15 23:28:41.552880 - dimDate table saved into local\n",
      "2021-11-15 23:28:41.663235 - question table updated - adding 3 date columns, rename 3 datetime columns\n",
      "2021-11-15 23:29:01.869898 - factQuestion table saved into local\n",
      "2021-11-15 23:29:02.082892 - file location config read successful\n",
      "2021-11-15 23:29:02.827004 - dimTag table created\n",
      "2021-11-15 23:29:02.858865 - dimTag table saved into local\n",
      "2021-11-15 23:29:07.126684 - factQuestionTag table updated and saved into local\n"
     ]
    }
   ],
   "source": [
    "config = read_config_file()\n",
    "download_data(config, 'question')\n",
    "download_data(config, 'question_tag')\n",
    "\n",
    "add_hash_column(config, 'question')\n",
    "add_hash_column(config, 'question_tag')\n",
    "\n",
    "transform_question_data(config)\n",
    "transform_question_tag_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_data_to_s3(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
